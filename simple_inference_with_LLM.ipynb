{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "login(token=userdata.get('HF_READ_TOKEN'))"
      ],
      "metadata": {
        "id": "lRr7NZFRoYq9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0E4gbPhvXms",
        "outputId": "b32f7b90-962c-4072-f89b-f3f88ea6eef4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=a934f302707923d22f681d26635b4ce90ca9f1be807124bbf3ee252f1a551fe1\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Testing script for base Qwen/Qwen3-4B-2507 model on WebQSP dataset\n",
        "Optimized for CUDA/Google Colab environment\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "import numpy as np\n",
        "from typing import Dict, List\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Set environment variables for optimization\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "\n",
        "class WebQSPTestDataset(Dataset):\n",
        "    \"\"\"Dataset class for WebQSP test data\"\"\"\n",
        "\n",
        "    def __init__(self, json_file: str, max_length: int = 512):\n",
        "        self.max_length = max_length\n",
        "        self.data = self.load_and_process_data(json_file)\n",
        "\n",
        "    def load_and_process_data(self, json_file: str) -> List[Dict]:\n",
        "        \"\"\"Load and process the WebQSP JSON file\"\"\"\n",
        "        with open(json_file, 'r', encoding='utf-8') as f:\n",
        "            raw_data = json.load(f)\n",
        "\n",
        "        processed_data = []\n",
        "\n",
        "        # Process the data format: list of dicts with question, reasoning_graph, answer\n",
        "        for idx, item in enumerate(raw_data):\n",
        "            question = item.get('question', '').strip()\n",
        "            reasoning_graph = item.get('reasoning_graph', [])\n",
        "            answer = item.get('answer', [])\n",
        "\n",
        "            # Skip if no question or answer\n",
        "            if not question or not answer:\n",
        "                continue\n",
        "\n",
        "            # Format reasoning graph as a string\n",
        "            if reasoning_graph:\n",
        "                # Handle nested lists and ensure all items are strings\n",
        "                flat_graph = []\n",
        "                for graph_item in reasoning_graph:\n",
        "                    if isinstance(graph_item, list):\n",
        "                        flat_graph.extend(str(x) for x in graph_item)\n",
        "                    else:\n",
        "                        flat_graph.append(str(graph_item))\n",
        "                reasoning_str = \" -> \".join(flat_graph)\n",
        "            else:\n",
        "                reasoning_str = \"\"\n",
        "\n",
        "            # Format answers as list of strings\n",
        "            if isinstance(answer, list):\n",
        "                answer_list = [str(ans) for ans in answer if ans]\n",
        "            else:\n",
        "                answer_list = [str(answer)]\n",
        "\n",
        "            if question and answer_list:\n",
        "                processed_data.append({\n",
        "                    'question': question,\n",
        "                    'reasoning_graph': reasoning_str,\n",
        "                    'ground_truth_answers': answer_list,\n",
        "                    'question_id': f\"test_{idx}\"\n",
        "                })\n",
        "\n",
        "        print(f\"Loaded {len(processed_data)} test examples\")\n",
        "        return processed_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "def load_model_and_tokenizer(\n",
        "    model_name: str,\n",
        "    use_4bit: bool = True\n",
        "):\n",
        "    \"\"\"Load the model and tokenizer\"\"\"\n",
        "\n",
        "    print(f\"Loading tokenizer from {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "        trust_remote_code=True,\n",
        "        padding_side='right'\n",
        "    )\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    if use_4bit:\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "        )\n",
        "\n",
        "        print(f\"Loading model with 4-bit quantization from {model_name}\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.float16,\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Loading model in FP16 from {model_name}\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.float16,\n",
        "        )\n",
        "\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def create_prompt(question: str, reasoning_graph: str) -> str:\n",
        "    \"\"\"Create prompt for the model\"\"\"\n",
        "    prompt = f\"\"\"<|im_start|>system\n",
        "You are a helpful assistant who answers questions using knowledge graph reasoning. You only use the information obtained from the context provided by the user. If you don't know the answer, just say that you don't know.\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "Question: {question}\n",
        "Reasoning Graph: {reasoning_graph}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def generate_answer(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    question: str,\n",
        "    reasoning_graph: str,\n",
        "    max_new_tokens: int = 20,\n",
        "    temperature: float = 0.7,\n",
        "    top_p: float = 0.9,\n",
        ") -> str:\n",
        "    \"\"\"Generate answer for a given question and reasoning graph\"\"\"\n",
        "\n",
        "    prompt = create_prompt(question, reasoning_graph)\n",
        "\n",
        "    # Tokenize and move inputs to the model device\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    input_ids = inputs['input_ids']\n",
        "    input_len = input_ids.shape[1]\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate and decode only the new tokens (slice off the prompt)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            top_p=top_p,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # outputs is (batch_size, seq_len); slice off prompt tokens\n",
        "    gen_ids = outputs[0]\n",
        "    if gen_ids.dim() == 2:\n",
        "        gen_ids = gen_ids[0]\n",
        "\n",
        "    # If generation includes the prompt, remove prompt tokens\n",
        "    if gen_ids.shape[0] > input_len:\n",
        "        new_tokens = gen_ids[input_len:]\n",
        "    else:\n",
        "        new_tokens = gen_ids\n",
        "\n",
        "    # Decode only generated tokens and remove special tokens\n",
        "    generated_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "    # Extract assistant response (Qwen format) from generated_text\n",
        "    response = generated_text\n",
        "    # remove any leading header markers if present\n",
        "    if \"<|im_start|>assistant\" in response:\n",
        "        response = response.split(\"<|im_start|>assistant\")[-1]\n",
        "    if \"<|im_end|>\" in response:\n",
        "        response = response.split(\"<|im_end|>\")[0]\n",
        "\n",
        "    response = response.strip()\n",
        "    return response\n",
        "\n",
        "\n",
        "def calculate_bleu_score(prediction: str, ground_truths: List[str]) -> float:\n",
        "    \"\"\"Calculate BLEU score\"\"\"\n",
        "    if not prediction.strip():\n",
        "        return 0.0\n",
        "\n",
        "    # Tokenize prediction\n",
        "    pred_tokens = prediction.lower().split()\n",
        "\n",
        "    # Tokenize all ground truths (BLEU expects list of reference token lists)\n",
        "    reference_tokens = [gt.lower().split() for gt in ground_truths]\n",
        "\n",
        "    # Use smoothing function to handle edge cases\n",
        "    smoothing = SmoothingFunction().method1\n",
        "\n",
        "    try:\n",
        "        # Calculate BLEU score (using BLEU-4)\n",
        "        bleu = sentence_bleu(reference_tokens, pred_tokens, smoothing_function=smoothing)\n",
        "        return bleu\n",
        "    except Exception as e:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def calculate_f1_score(prediction: str, ground_truths: List[str]) -> float:\n",
        "    \"\"\"Calculate F1 score based on token overlap\"\"\"\n",
        "    if not prediction.strip():\n",
        "        return 0.0\n",
        "\n",
        "    pred_tokens = set(prediction.lower().split())\n",
        "\n",
        "    max_f1 = 0.0\n",
        "    for gt in ground_truths:\n",
        "        gt_tokens = set(gt.lower().split())\n",
        "\n",
        "        if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
        "            continue\n",
        "\n",
        "        common_tokens = pred_tokens & gt_tokens\n",
        "        if len(common_tokens) == 0:\n",
        "            continue\n",
        "\n",
        "        precision = len(common_tokens) / len(pred_tokens)\n",
        "        recall = len(common_tokens) / len(gt_tokens)\n",
        "        f1 = 2 * (precision * recall) / (precision + recall)\n",
        "        max_f1 = max(max_f1, f1)\n",
        "\n",
        "    return max_f1\n",
        "\n",
        "\n",
        "def calculate_rouge_scores(prediction: str, ground_truths: List[str]) -> Dict[str, float]:\n",
        "    \"\"\"Calculate ROUGE-1, ROUGE-2, and ROUGE-L scores with recall, precision, and F-measure\"\"\"\n",
        "    if not prediction.strip():\n",
        "        return {\n",
        "            'rouge1_recall': 0.0, 'rouge1_precision': 0.0, 'rouge1_fmeasure': 0.0,\n",
        "            'rouge2_recall': 0.0, 'rouge2_precision': 0.0, 'rouge2_fmeasure': 0.0,\n",
        "            'rougeL_recall': 0.0, 'rougeL_precision': 0.0, 'rougeL_fmeasure': 0.0\n",
        "        }\n",
        "\n",
        "    # Initialize ROUGE scorer\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    # Calculate ROUGE scores against all ground truths and take the maximum for each metric\n",
        "    max_scores = {\n",
        "        'rouge1_recall': 0.0, 'rouge1_precision': 0.0, 'rouge1_fmeasure': 0.0,\n",
        "        'rouge2_recall': 0.0, 'rouge2_precision': 0.0, 'rouge2_fmeasure': 0.0,\n",
        "        'rougeL_recall': 0.0, 'rougeL_precision': 0.0, 'rougeL_fmeasure': 0.0\n",
        "    }\n",
        "\n",
        "    for gt in ground_truths:\n",
        "        scores = scorer.score(gt, prediction)\n",
        "\n",
        "        # ROUGE-1 scores\n",
        "        max_scores['rouge1_recall'] = max(max_scores['rouge1_recall'], scores['rouge1'].recall)\n",
        "        max_scores['rouge1_precision'] = max(max_scores['rouge1_precision'], scores['rouge1'].precision)\n",
        "        max_scores['rouge1_fmeasure'] = max(max_scores['rouge1_fmeasure'], scores['rouge1'].fmeasure)\n",
        "\n",
        "        # ROUGE-2 scores\n",
        "        max_scores['rouge2_recall'] = max(max_scores['rouge2_recall'], scores['rouge2'].recall)\n",
        "        max_scores['rouge2_precision'] = max(max_scores['rouge2_precision'], scores['rouge2'].precision)\n",
        "        max_scores['rouge2_fmeasure'] = max(max_scores['rouge2_fmeasure'], scores['rouge2'].fmeasure)\n",
        "\n",
        "        # ROUGE-L scores\n",
        "        max_scores['rougeL_recall'] = max(max_scores['rougeL_recall'], scores['rougeL'].recall)\n",
        "        max_scores['rougeL_precision'] = max(max_scores['rougeL_precision'], scores['rougeL'].precision)\n",
        "        max_scores['rougeL_fmeasure'] = max(max_scores['rougeL_fmeasure'], scores['rougeL'].fmeasure)\n",
        "\n",
        "    return max_scores\n",
        "\n",
        "\n",
        "def test_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    dataset: WebQSPTestDataset,\n",
        "    output_file: str = \"base_model_test_results.json\",\n",
        "    num_samples: int = None,\n",
        "    save_predictions: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"Test the model on test dataset\"\"\"\n",
        "\n",
        "    results = []\n",
        "    metrics = {\n",
        "        'bleu': [],\n",
        "        'f1': [],\n",
        "        'rouge1_recall': [], 'rouge1_precision': [], 'rouge1_fmeasure': [],\n",
        "        'rouge2_recall': [], 'rouge2_precision': [], 'rouge2_fmeasure': [],\n",
        "        'rougeL_recall': [], 'rougeL_precision': [], 'rougeL_fmeasure': []\n",
        "    }\n",
        "\n",
        "    num_samples = num_samples or len(dataset)\n",
        "    num_samples = min(num_samples, len(dataset))\n",
        "\n",
        "    print(f\"\\nTesting on {num_samples} samples...\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx in tqdm(range(num_samples), desc=\"Testing\"):\n",
        "        item = dataset[idx]\n",
        "\n",
        "        try:\n",
        "            prediction = generate_answer(\n",
        "                model,\n",
        "                tokenizer,\n",
        "                item['question'],\n",
        "                item['reasoning_graph'],\n",
        "                max_new_tokens=100,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError generating answer for sample {idx}: {e}\")\n",
        "            prediction = \"\"\n",
        "\n",
        "        ground_truths = item['ground_truth_answers']\n",
        "\n",
        "        # Calculate metrics\n",
        "        bleu = calculate_bleu_score(prediction, ground_truths)\n",
        "        f1 = calculate_f1_score(prediction, ground_truths)\n",
        "        rouge_scores = calculate_rouge_scores(prediction, ground_truths)\n",
        "\n",
        "        metrics['bleu'].append(bleu)\n",
        "        metrics['f1'].append(f1)\n",
        "        metrics['rouge1_recall'].append(rouge_scores['rouge1_recall'])\n",
        "        metrics['rouge1_precision'].append(rouge_scores['rouge1_precision'])\n",
        "        metrics['rouge1_fmeasure'].append(rouge_scores['rouge1_fmeasure'])\n",
        "        metrics['rouge2_recall'].append(rouge_scores['rouge2_recall'])\n",
        "        metrics['rouge2_precision'].append(rouge_scores['rouge2_precision'])\n",
        "        metrics['rouge2_fmeasure'].append(rouge_scores['rouge2_fmeasure'])\n",
        "        metrics['rougeL_recall'].append(rouge_scores['rougeL_recall'])\n",
        "        metrics['rougeL_precision'].append(rouge_scores['rougeL_precision'])\n",
        "        metrics['rougeL_fmeasure'].append(rouge_scores['rougeL_fmeasure'])\n",
        "\n",
        "        # Store result\n",
        "        result = {\n",
        "            'question_id': item['question_id'],\n",
        "            'question': item['question'],\n",
        "            'reasoning_graph': item['reasoning_graph'],\n",
        "            'ground_truth': ground_truths,\n",
        "            'prediction': prediction,\n",
        "            'bleu_score': bleu,\n",
        "            'f1_score': f1,\n",
        "            'rouge1_recall': rouge_scores['rouge1_recall'],\n",
        "            'rouge1_precision': rouge_scores['rouge1_precision'],\n",
        "            'rouge1_fmeasure': rouge_scores['rouge1_fmeasure'],\n",
        "            'rouge2_recall': rouge_scores['rouge2_recall'],\n",
        "            'rouge2_precision': rouge_scores['rouge2_precision'],\n",
        "            'rouge2_fmeasure': rouge_scores['rouge2_fmeasure'],\n",
        "            'rougeL_recall': rouge_scores['rougeL_recall'],\n",
        "            'rougeL_precision': rouge_scores['rougeL_precision'],\n",
        "            'rougeL_fmeasure': rouge_scores['rougeL_fmeasure']\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        # Print sample results every 100 samples\n",
        "        if (idx + 1) % 100 == 0:\n",
        "            print(f\"\\n--- Sample {idx + 1} ---\")\n",
        "            print(f\"Question: {item['question']}\")\n",
        "            print(f\"Prediction: {prediction}\")\n",
        "            print(f\"Ground Truth: {ground_truths}\")\n",
        "            print(f\"BLEU: {bleu:.3f}, F1: {f1:.3f}, ROUGE-L F1: {rouge_scores['rougeL_fmeasure']:.3f}\")\n",
        "\n",
        "    test_time = time.time() - start_time\n",
        "\n",
        "    # Calculate aggregate metrics\n",
        "    aggregate_metrics = {\n",
        "        'bleu_score': np.mean(metrics['bleu']),\n",
        "        'f1_score': np.mean(metrics['f1']),\n",
        "        'rouge1_recall': np.mean(metrics['rouge1_recall']),\n",
        "        'rouge1_precision': np.mean(metrics['rouge1_precision']),\n",
        "        'rouge1_fmeasure': np.mean(metrics['rouge1_fmeasure']),\n",
        "        'rouge2_recall': np.mean(metrics['rouge2_recall']),\n",
        "        'rouge2_precision': np.mean(metrics['rouge2_precision']),\n",
        "        'rouge2_fmeasure': np.mean(metrics['rouge2_fmeasure']),\n",
        "        'rougeL_recall': np.mean(metrics['rougeL_recall']),\n",
        "        'rougeL_precision': np.mean(metrics['rougeL_precision']),\n",
        "        'rougeL_fmeasure': np.mean(metrics['rougeL_fmeasure']),\n",
        "        'total_samples': num_samples,\n",
        "        'test_time_minutes': test_time / 60,\n",
        "        'average_time_per_sample': test_time / num_samples\n",
        "    }\n",
        "\n",
        "    # Calculate additional statistics (standard deviations)\n",
        "    aggregate_metrics['bleu_score_std'] = np.std(metrics['bleu'])\n",
        "    aggregate_metrics['f1_score_std'] = np.std(metrics['f1'])\n",
        "    aggregate_metrics['rouge1_recall_std'] = np.std(metrics['rouge1_recall'])\n",
        "    aggregate_metrics['rouge1_precision_std'] = np.std(metrics['rouge1_precision'])\n",
        "    aggregate_metrics['rouge1_fmeasure_std'] = np.std(metrics['rouge1_fmeasure'])\n",
        "    aggregate_metrics['rouge2_recall_std'] = np.std(metrics['rouge2_recall'])\n",
        "    aggregate_metrics['rouge2_precision_std'] = np.std(metrics['rouge2_precision'])\n",
        "    aggregate_metrics['rouge2_fmeasure_std'] = np.std(metrics['rouge2_fmeasure'])\n",
        "    aggregate_metrics['rougeL_recall_std'] = np.std(metrics['rougeL_recall'])\n",
        "    aggregate_metrics['rougeL_precision_std'] = np.std(metrics['rougeL_precision'])\n",
        "    aggregate_metrics['rougeL_fmeasure_std'] = np.std(metrics['rougeL_fmeasure'])\n",
        "\n",
        "    # Save results to file\n",
        "    if save_predictions:\n",
        "        output_data = {\n",
        "            'aggregate_metrics': aggregate_metrics,\n",
        "            'detailed_results': results\n",
        "        }\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"\\nResults saved to {output_file}\")\n",
        "\n",
        "    return aggregate_metrics\n",
        "\n",
        "\n",
        "def print_test_summary(metrics: Dict):\n",
        "    \"\"\"Print test summary in a formatted way\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"BASE MODEL TEST RESULTS SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Total Samples:              {metrics['total_samples']}\")\n",
        "    print(f\"Test Time:                  {metrics['test_time_minutes']:.2f} minutes\")\n",
        "    print(f\"Avg Time per Sample:        {metrics['average_time_per_sample']:.2f} seconds\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"BLEU Score:                 {metrics['bleu_score']:.4f} ({metrics['bleu_score']*100:.2f}%)\")\n",
        "    print(f\"  ± Std Dev:                {metrics['bleu_score_std']:.4f}\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"F1 Score:                   {metrics['f1_score']:.4f} ({metrics['f1_score']*100:.2f}%)\")\n",
        "    print(f\"  ± Std Dev:                {metrics['f1_score_std']:.4f}\")\n",
        "    print(\"-\"*70)\n",
        "    print(\"ROUGE-1 Scores:\")\n",
        "    print(f\"  Recall:                   {metrics['rouge1_recall']:.4f} ({metrics['rouge1_recall']*100:.2f}%)\")\n",
        "    print(f\"    ± Std Dev:              {metrics['rouge1_recall_std']:.4f}\")\n",
        "    print(f\"  Precision:                {metrics['rouge1_precision']:.4f} ({metrics['rouge1_precision']*100:.2f}%)\")\n",
        "    print(f\"    ± Std Dev:              {metrics['rouge1_precision_std']:.4f}\")\n",
        "    print(f\"  F-measure:                {metrics['rouge1_fmeasure']:.4f} ({metrics['rouge1_fmeasure']*100:.2f}%)\")\n",
        "    print(f\"    ± Std Dev:              {metrics['rouge1_fmeasure_std']:.4f}\")\n",
        "    print(\"-\"*70)\n",
        "    print(\"ROUGE-2 Scores:\")\n",
        "    print(f\"  Recall:                   {metrics['rouge2_recall']:.4f} ({metrics['rouge2_recall']*100:.2f}%)\")\n",
        "    print(f\"    ± Std Dev:              {metrics['rouge2_recall_std']:.4f}\")\n",
        "    print(f\"  Precision:                {metrics['rouge2_precision']:.4f} ({metrics['rouge2_precision']*100:.2f}%)\")\n",
        "    print(f\"    ± Std Dev:              {metrics['rouge2_precision_std']:.4f}\")\n",
        "    print(f\"  F-measure:                {metrics['rouge2_fmeasure']:.4f} ({metrics['rouge2_fmeasure']*100:.2f}%)\")\n",
        "    print(f\"    ± Std Dev:              {metrics['rouge2_fmeasure_std']:.4f}\")\n",
        "    print(\"-\"*70)\n",
        "    print(\"ROUGE-L Scores:\")\n",
        "    print(f\"  Recall:                   {metrics['rougeL_recall']:.4f} ({metrics['rougeL_recall']*100:.2f}%)\")\n",
        "    print(f\"    ± Std Dev:              {metrics['rougeL_recall_std']:.4f}\")\n",
        "    print(f\"  Precision:                {metrics['rougeL_precision']:.4f} ({metrics['rougeL_precision']*100:.2f}%)\")\n",
        "    print(f\"    ± Std Dev:              {metrics['rougeL_precision_std']:.4f}\")\n",
        "    print(f\"  F-measure:                {metrics['rougeL_fmeasure']:.4f} ({metrics['rougeL_fmeasure']*100:.2f}%)\")\n",
        "    print(f\"    ± Std Dev:              {metrics['rougeL_fmeasure_std']:.4f}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "\n",
        "def analyze_results(results_file: str):\n",
        "    \"\"\"Analyze test results and provide insights\"\"\"\n",
        "\n",
        "    with open(results_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    results = data['detailed_results']\n",
        "\n",
        "    # Analyze performance patterns\n",
        "    # Sort by F1 score to find best and worst predictions\n",
        "    sorted_by_f1 = sorted(results, key=lambda x: x['f1_score'], reverse=True)\n",
        "    high_quality = sorted_by_f1[:len(sorted_by_f1)//3]  # Top 1/3\n",
        "    medium_quality = sorted_by_f1[len(sorted_by_f1)//3:2*len(sorted_by_f1)//3]  # Middle 1/3\n",
        "    low_quality = sorted_by_f1[2*len(sorted_by_f1)//3:]  # Bottom 1/3\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"DETAILED ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"High Quality Predictions (Top 33%):     {len(high_quality)}\")\n",
        "    print(f\"Medium Quality Predictions (Mid 33%):   {len(medium_quality)}\")\n",
        "    print(f\"Low Quality Predictions (Bottom 33%):   {len(low_quality)}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    # Show some examples\n",
        "    if high_quality:\n",
        "        print(\"\\n✓ Sample High Quality Prediction:\")\n",
        "        sample = high_quality[0]\n",
        "        print(f\"  Question: {sample['question']}\")\n",
        "        print(f\"  Prediction: {sample['prediction']}\")\n",
        "        print(f\"  Ground Truth: {sample['ground_truth']}\")\n",
        "        print(f\"  Scores - BLEU: {sample['bleu_score']:.3f}, F1: {sample['f1_score']:.3f}, ROUGE-L F1: {sample['rougeL_fmeasure']:.3f}\")\n",
        "\n",
        "    if medium_quality:\n",
        "        print(\"\\n≈ Sample Medium Quality Prediction:\")\n",
        "        sample = medium_quality[len(medium_quality)//2]\n",
        "        print(f\"  Question: {sample['question']}\")\n",
        "        print(f\"  Prediction: {sample['prediction']}\")\n",
        "        print(f\"  Ground Truth: {sample['ground_truth']}\")\n",
        "        print(f\"  Scores - BLEU: {sample['bleu_score']:.3f}, F1: {sample['f1_score']:.3f}, ROUGE-L F1: {sample['rougeL_fmeasure']:.3f}\")\n",
        "\n",
        "    if low_quality:\n",
        "        print(\"\\n✗ Sample Low Quality Prediction:\")\n",
        "        sample = low_quality[0]\n",
        "        print(f\"  Question: {sample['question']}\")\n",
        "        print(f\"  Prediction: {sample['prediction']}\")\n",
        "        print(f\"  Ground Truth: {sample['ground_truth']}\")\n",
        "        print(f\"  Scores - BLEU: {sample['bleu_score']:.3f}, F1: {sample['f1_score']:.3f}, ROUGE-L F1: {sample['rougeL_fmeasure']:.3f}\")\n",
        "\n",
        "    print(\"=\"*70)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main testing pipeline\"\"\"\n",
        "\n",
        "    # Configuration\n",
        "    CONFIG = {\n",
        "        'model_name': \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "        'test_file': \"webqsp_test_validated.json\",\n",
        "        'output_file': \"llama_1B_test_exhops_results.json\",\n",
        "        'use_4bit': False,\n",
        "        'num_samples': None,  # None = all samples\n",
        "    }\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"QWEN BASE MODEL TESTING PIPELINE\")\n",
        "    print(f\"Model: {CONFIG['model_name']}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Check CUDA\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    else:\n",
        "        print(\"WARNING: CUDA not available, testing will be slow on CPU\")\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    model, tokenizer = load_model_and_tokenizer(\n",
        "        CONFIG['model_name'],\n",
        "        use_4bit=CONFIG['use_4bit']\n",
        "    )\n",
        "\n",
        "    # Load test dataset\n",
        "    print(f\"\\nLoading test dataset from {CONFIG['test_file']}\")\n",
        "    test_dataset = WebQSPTestDataset(CONFIG['test_file'])\n",
        "\n",
        "    # Clear CUDA cache\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Run testing\n",
        "    metrics = test_model(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        test_dataset,\n",
        "        output_file=CONFIG['output_file'],\n",
        "        num_samples=CONFIG['num_samples']\n",
        "    )\n",
        "\n",
        "    # Print summary\n",
        "    print_test_summary(metrics)\n",
        "\n",
        "    # Analyze results\n",
        "    analyze_results(CONFIG['output_file'])\n",
        "\n",
        "    print(f\"\\n✓ Testing completed successfully!\")\n",
        "    print(f\"  Results saved to: {CONFIG['output_file']}\")\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNqHYEHcnui5",
        "outputId": "2fa3e3d1-a960-4b52-fdf0-1db79b5ce0ca"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "QWEN BASE MODEL TESTING PIPELINE\n",
            "Model: meta-llama/Llama-3.2-1B-Instruct\n",
            "======================================================================\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "GPU Memory: 85.17 GB\n",
            "Loading tokenizer from meta-llama/Llama-3.2-1B-Instruct\n",
            "Loading model in FP16 from meta-llama/Llama-3.2-1B-Instruct\n",
            "\n",
            "Loading test dataset from webqsp_test_validated.json\n",
            "Loaded 854 test examples\n",
            "\n",
            "Testing on 854 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing:  12%|█▏        | 100/854 [01:27<14:53,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample 100 ---\n",
            "Question: what did fred durst do?\n",
            "Prediction: According to the provided context, Fred Durst is a famous American musician and singer who has been involved in various roles, including being a member of the band Limp Bizkit and starring in a TV role. He was born in Jacksonville, Florida, and played the guitar. He has also been involved in other roles, such as being a professional musician, artist, and actor.\n",
            "Ground Truth: ['Actor', 'Musician', 'Rapper', 'Guitarist', 'Film director', 'Businessperson', 'Songwriter', 'Singer']\n",
            "BLEU: 0.003, F1: 0.043, ROUGE-L F1: 0.031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing:  23%|██▎       | 200/854 [03:10<13:00,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample 200 ---\n",
            "Question: what currency is used in panama?\n",
            "Prediction: The currency used in Panama is the United States Dollar (USD).\n",
            "Ground Truth: ['United States Dollar', 'Panamanian balboa']\n",
            "BLEU: 0.093, F1: 0.462, ROUGE-L F1: 0.429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing:  35%|███▌      | 300/854 [04:52<04:17,  2.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample 300 ---\n",
            "Question: what team is reggie bush on 2011?\n",
            "Prediction: Based on the provided reasoning graph, Reggie Bush is a football player who was drafted by the New Orleans Saints in the 2005 NFL Draft.\n",
            "Ground Truth: ['Miami Dolphins', 'New Orleans Saints']\n",
            "BLEU: 0.037, F1: 0.231, ROUGE-L F1: 0.214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing:  47%|████▋     | 400/854 [06:23<07:50,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample 400 ---\n",
            "Question: what county is san diego ca?\n",
            "Prediction: The county is San Diego County.\n",
            "Ground Truth: ['San Diego County']\n",
            "BLEU: 0.096, F1: 0.667, ROUGE-L F1: 0.667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing:  59%|█████▊    | 500/854 [08:04<05:40,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample 500 ---\n",
            "Question: where did the greek alphabet come from?\n",
            "Prediction: The Greek alphabet is believed to have originated from the Phoenician alphabet, which was used by the Phoenicians to write various languages, including Greek. The Phoenician alphabet is thought to have been created around 1500 BCE and consisted of 22 characters. Over time, the Phoenician alphabet evolved and was adapted by the Greeks to create their own writing system. The Greek alphabet is unique in that it is the first alphabet to have 24 characters, and it is still used today\n",
            "Ground Truth: ['Phoenician alphabet']\n",
            "BLEU: 0.005, F1: 0.075, ROUGE-L F1: 0.048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing:  70%|███████   | 601/854 [09:40<02:22,  1.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample 600 ---\n",
            "Question: what kind of government is south korea?\n",
            "Prediction: Based on the provided information, I would say that South Korea is a unitary state with a presidential system.\n",
            "Ground Truth: ['Constitutional republic', 'Presidential system', 'Unitary state']\n",
            "BLEU: 0.024, F1: 0.200, ROUGE-L F1: 0.190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing:  82%|████████▏ | 700/854 [11:09<01:52,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample 700 ---\n",
            "Question: who did ben stiller date?\n",
            "Prediction: Ben Stiller dated Amanda Peet.\n",
            "Ground Truth: ['Calista Flockhart', 'Amanda Peet', 'Jeanne Tripplehorn', 'Janeane Garofalo', 'Christine Taylor', 'Claire Forlani', 'Rhea Durham']\n",
            "BLEU: 0.054, F1: 0.286, ROUGE-L F1: 0.571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing:  94%|█████████▎| 800/854 [12:48<00:33,  1.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample 800 ---\n",
            "Question: who was william mckinley married to?\n",
            "Prediction: Based on the information provided, Ida McKinley was William McKinley's wife.\n",
            "Ground Truth: ['Ida Saxton McKinley']\n",
            "BLEU: 0.022, F1: 0.286, ROUGE-L F1: 0.267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 854/854 [13:37<00:00,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results saved to llama_1B_test_exhops_results.json\n",
            "\n",
            "======================================================================\n",
            "BASE MODEL TEST RESULTS SUMMARY\n",
            "======================================================================\n",
            "Total Samples:              854\n",
            "Test Time:                  13.63 minutes\n",
            "Avg Time per Sample:        0.96 seconds\n",
            "----------------------------------------------------------------------\n",
            "BLEU Score:                 0.0191 (1.91%)\n",
            "  ± Std Dev:                0.0389\n",
            "----------------------------------------------------------------------\n",
            "F1 Score:                   0.1172 (11.72%)\n",
            "  ± Std Dev:                0.1365\n",
            "----------------------------------------------------------------------\n",
            "ROUGE-1 Scores:\n",
            "  Recall:                   0.7823 (78.23%)\n",
            "    ± Std Dev:              0.3885\n",
            "  Precision:                0.1034 (10.34%)\n",
            "    ± Std Dev:              0.1060\n",
            "  F-measure:                0.1711 (17.11%)\n",
            "    ± Std Dev:              0.1584\n",
            "----------------------------------------------------------------------\n",
            "ROUGE-2 Scores:\n",
            "  Recall:                   0.4490 (44.90%)\n",
            "    ± Std Dev:              0.4912\n",
            "  Precision:                0.0467 (4.67%)\n",
            "    ± Std Dev:              0.0769\n",
            "  F-measure:                0.0800 (8.00%)\n",
            "    ± Std Dev:              0.1227\n",
            "----------------------------------------------------------------------\n",
            "ROUGE-L Scores:\n",
            "  Recall:                   0.7605 (76.05%)\n",
            "    ± Std Dev:              0.3902\n",
            "  Precision:                0.0986 (9.86%)\n",
            "    ± Std Dev:              0.1022\n",
            "  F-measure:                0.1634 (16.34%)\n",
            "    ± Std Dev:              0.1526\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "DETAILED ANALYSIS\n",
            "======================================================================\n",
            "High Quality Predictions (Top 33%):     284\n",
            "Medium Quality Predictions (Mid 33%):   285\n",
            "Low Quality Predictions (Bottom 33%):   285\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "✓ Sample High Quality Prediction:\n",
            "  Question: what did jesse owens won?\n",
            "  Prediction: Jesse Owens won the Associated Press Male Athlete of the Year award.\n",
            "  Ground Truth: ['Associated Press Male Athlete of the Year']\n",
            "  Scores - BLEU: 0.516, F1: 0.778, ROUGE-L F1: 0.737\n",
            "\n",
            "≈ Sample Medium Quality Prediction:\n",
            "  Question: where did the greek alphabet come from?\n",
            "  Prediction: The Greek alphabet is believed to have originated from the Phoenician alphabet, which was used by the Phoenicians to write various languages, including Greek. The Phoenician alphabet is thought to have been created around 1500 BCE and consisted of 22 characters. Over time, the Phoenician alphabet evolved and was adapted by the Greeks to create their own writing system. The Greek alphabet is unique in that it is the first alphabet to have 24 characters, and it is still used today\n",
            "  Ground Truth: ['Phoenician alphabet']\n",
            "  Scores - BLEU: 0.005, F1: 0.075, ROUGE-L F1: 0.048\n",
            "\n",
            "✗ Sample Low Quality Prediction:\n",
            "  Question: who is jimmy savile?\n",
            "  Prediction: I don't know the answer to who Jimmy Savile was.\n",
            "  Ground Truth: ['Radio personality', 'Presenter', 'Disc jockey']\n",
            "  Scores - BLEU: 0.000, F1: 0.000, ROUGE-L F1: 0.000\n",
            "======================================================================\n",
            "\n",
            "✓ Testing completed successfully!\n",
            "  Results saved to: llama_1B_test_exhops_results.json\n",
            "\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_"
      ],
      "metadata": {
        "id": "FVu03sWDu6rJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}